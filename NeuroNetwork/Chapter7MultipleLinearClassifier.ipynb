{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "file_name = \"ch07.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetType(Enum):\n",
    "    Fitting = (1,)\n",
    "    BinaryClassifier = (2,)\n",
    "    MultipleClassifier = (3,)\n",
    "    BinaryTanh = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HyperParameters(object):\n",
    "    def __init__(\n",
    "        self, input_size, output_size, eta=0.1, max_epoch=1000, batch_size=5, eps=0.1,net_type=NetType.Fitting\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.eta = eta\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.eps = eps\n",
    "        self.net_type = net_type\n",
    "\n",
    "    def toString(self):\n",
    "        title = str.format(\"bz:{0},eta:{1}\", self.batch_size, self.eta)\n",
    "        return title\n",
    "\n",
    "class LossFunction(object):\n",
    "    def _init_(self,net_type):\n",
    "        self.net_type = net_type\n",
    "\n",
    "   # fcFunc: feed forward calculation\n",
    "    def CheckLoss(self, A, Y):\n",
    "        m = Y.shape[0]\n",
    "        if self.net_type == NetType.BinaryClassifier:\n",
    "            loss = self.CE2(A, Y, m)\n",
    "        elif self.net_type == NetType.BinaryTanh:\n",
    "            loss = self.CE2_tanh(A, Y, m)\n",
    "        elif self.net_type== NetType.MultipleClassifier:\n",
    "            loss= self.CE3(A,Y,m)\n",
    "\n",
    "        return loss\n",
    "  \n",
    "    def CE3(self,A,Y,count):\n",
    "    def MSE(self,A,Y,count):\n",
    "        LOSS = (A - Y)**2\n",
    "        loss = LOSS.sum()/count/2\n",
    "        return loss\n",
    "    \n",
    "    def CE2(self, A, Y, count):\n",
    "        p1 = 1 - Y\n",
    "        p2 = np.log(1 - A)\n",
    "        p3 = np.log(A)\n",
    "\n",
    "        p4 = np.multiply(p1 ,p2)\n",
    "        p5 = np.multiply(Y, p3)\n",
    "\n",
    "        LOSS = np.sum(-(p4 + p5))  #binary classification\n",
    "        loss = LOSS / count\n",
    "        return loss\n",
    "    # end def\n",
    "\n",
    "    # for binary tanh classifier\n",
    "    def CE2_tanh(self, A, Y, count):\n",
    "        #p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)\n",
    "        p = (1-Y) * np.log((1-A)/2) + (1+Y) * np.log((1+A)/2)\n",
    "        LOSS = np.sum(-p)\n",
    "        loss = LOSS / count\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory_1_0(object):\n",
    "    def __init__(self):\n",
    "        self.iteration = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def AddLossHistory(self, iteration, loss):\n",
    "        self.iteration.append(iteration)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "    def ShowLossHistory(self, hp, xmin=None, xmax=None, ymin=None, ymax=None):\n",
    "        plt.plot(self.iteration, self.loss_history)\n",
    "        title = hp.toString()\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        if xmin != None and ymin != None:\n",
    "            plt.axis([xmin, xmax, ymin, ymax])\n",
    "        plt.show()\n",
    "        return title\n",
    "\n",
    "    def GetLast(self):\n",
    "        count = len(self.loss_history)\n",
    "        return self.loss_history[count-1], self.w_history[count-1], self.b_history[count-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "        self.W = np.zeros(\n",
    "            (self.hp.input_size, self.hp.output_size)\n",
    "        )  # 权重矩阵由神经网络输入输出个数决定\n",
    "        self.B = np.zeros((1, self.hp.output_size))\n",
    "\n",
    "    # 多样本计算\n",
    "    def __forwardBatch(self, batch_x):\n",
    "        Z = np.dot(batch_x, self.W) + self.B\n",
    "        if self.hp.net_type == NetType.BinaryClassifier:\n",
    "            A = Logistic().forward(Z)\n",
    "            return A\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "    def __backwardBatch(self, batch_x, batch_y, batch_z):\n",
    "        m = batch_x.shape[0]\n",
    "        dZ = batch_z - batch_y\n",
    "        dB = dZ.sum(axis=0, keepdims=True) / m\n",
    "        dW = np.dot(batch_x.T, dZ) / m\n",
    "        return dW, dB\n",
    "\n",
    "    def __update(self, dW, dB):\n",
    "        self.W = self.W - self.hp.eta * dW\n",
    "        self.B = self.B - self.hp.eta * dB\n",
    "\n",
    "    def inference(self, x):\n",
    "        return self.__forwardBatch(x)\n",
    "\n",
    "    def __checkLoss(self, dataReader):  # 多样本损失函数计算\n",
    "        X, Y = dataReader.GetWholeTrainSamples()\n",
    "        m = X.shape[0]\n",
    "        Z = self.__forwardBatch(X)\n",
    "        LOSS = (Z - Y) ** 2\n",
    "        loss = LOSS.sum() / m / 2\n",
    "        return loss\n",
    "\n",
    "    # 训练\n",
    "    def train(self, dataReader, checkpoint, net_type):\n",
    "        # calculate loss to decide the stop condition\n",
    "        loss_history = TrainingHistory_1_0()\n",
    "        loss = 10\n",
    "        if self.hp.batch_size == -1:\n",
    "            self.hp.batch_size = dataReader.num_train\n",
    "        max_iteration = math.ceil(dataReader.num_train / self.hp.batch_size)\n",
    "        checkpoint_iteration = (int)(max_iteration * checkpoint)\n",
    "\n",
    "        for epoch in range(self.hp.max_epoch):\n",
    "            print(\"epoch=%d\" % epoch)\n",
    "            dataReader.Shuffle()\n",
    "            for iteration in range(max_iteration):\n",
    "                # get x and y value for one sample\n",
    "                batch_x, batch_y = dataReader.GetBatchTrainSamples(\n",
    "                    self.hp.batch_size, iteration\n",
    "                )\n",
    "                # get z from x,y\n",
    "                batch_z = self.__forwardBatch(batch_x)\n",
    "                # calculate gradient of w and b\n",
    "                dW, dB = self.__backwardBatch(batch_x, batch_y, batch_z)\n",
    "                # update w,b\n",
    "                self.__update(dW, dB)\n",
    "\n",
    "                total_iteration = epoch * max_iteration + iteration\n",
    "                if (total_iteration + 1) % checkpoint_iteration == 0:\n",
    "                    loss = self.__checkLoss(dataReader)\n",
    "                    print(epoch, iteration, loss, self.W, self.B)\n",
    "                    loss_history.AddLossHistory(epoch * max_iteration + iteration, loss)\n",
    "                    if loss < self.hp.eps:\n",
    "                        break\n",
    "                    # end if\n",
    "                # end if\n",
    "            # end for\n",
    "            if loss < self.hp.eps:\n",
    "                break\n",
    "        # end for\n",
    "        loss_history.ShowLossHistory(self.hp)\n",
    "        print(\"W=\", self.W)\n",
    "        print(\"B=\", self.B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeruralNetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
